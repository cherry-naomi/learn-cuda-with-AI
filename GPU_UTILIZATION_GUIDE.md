# GPU Utilization Optimization Guide for CUDA Softmax

## Overview
This guide explains how to optimize `blocks_per_grid` and `threads_per_block` to fully utilize GPU resources for the `softmax_optimized` kernel.

## Current Results (A100 GPU)
- **Configuration**: batch_size=49,152, dim=1,024
- **Optimal Settings**: 1024 threads/block, 49,152 blocks
- **GPU Occupancy**: 800% (8 blocks per SM)
- **Performance**: 159.46x speedup over CPU

## Key Principles

### 1. Threads per Block Optimization

#### For A100 GPU:
- **Maximum**: 1024 threads per block (32 warps)
- **Optimal based on dimension**:
  ```cpp
  if (dim <= 256)     threads_per_block = 256;  // 8 warps
  else if (dim <= 512) threads_per_block = 512;  // 16 warps  
  else if (dim <= 1024) threads_per_block = 1024; // 32 warps (max)
  else threads_per_block = 1024; // Cap at maximum
  ```

#### Why 1024 threads for dim=1024?
- Each thread can process exactly 1 element (dim=1024, threads=1024)
- Maximum warp utilization (32 warps per block)
- Optimal shared memory usage (4KB per block)

### 2. Blocks per Grid Optimization

#### For Softmax (one block per batch element):
```cpp
blocks_per_grid = batch_size; // Each block handles one row
```

#### A100 SM Limits:
- **Max blocks per SM**: 16
- **Max threads per SM**: 2048
- **Total SMs**: 108

#### Occupancy Calculation:
```cpp
blocks_per_sm = 2048 / threads_per_block;
blocks_per_sm = min(blocks_per_sm, 16); // Cap at SM limit
total_blocks = min(batch_size, 108 * 16); // 1728 max blocks
occupancy = total_blocks / (blocks_per_sm * 108);
```

### 3. GPU Architecture Considerations

#### A100 Specifics:
- **Compute Capability**: 8.0
- **SMs**: 108
- **CUDA Cores per SM**: 64
- **Shared Memory per SM**: 164 KB
- **L2 Cache**: 40 MB

#### Memory Hierarchy:
1. **Registers**: Fastest, per-thread
2. **Shared Memory**: Block-level, 164 KB/SM
3. **L1 Cache**: 128 KB/SM
4. **L2 Cache**: 40 MB (shared)
5. **Global Memory**: 40 GB (slowest)

## Optimization Strategies

### 1. Maximize Occupancy
- Use maximum threads per block (1024 for A100)
- Ensure enough blocks to fill all SMs
- Target >50% occupancy for good performance

### 2. Memory Bandwidth Optimization
- **Current**: 192 MB data, 2.45 ms â†’ ~78 GB/s
- **Peak**: 777.6 GB/s
- **Utilization**: ~10% (memory-bound)

### 3. Compute Utilization
- **Current**: ~0.3% compute utilization
- **Peak**: 9.75 TFLOPS
- **Bottleneck**: Memory bandwidth, not compute

## Performance Analysis

### Current Configuration (Optimal):
```
Threads per block: 1024 (32.0 warps)
Blocks per grid: 49152
Active SMs: 864 / 108 (800% occupancy)
GPU time: 2.4547 ms
Memory bandwidth: ~78 GB/s
Speedup: 159.46x
```

### Why This is Optimal:
1. **Perfect Thread Mapping**: 1024 threads = 1024 elements per row
2. **Maximum Occupancy**: 8 blocks per SM (vs max 16)
3. **Efficient Warp Usage**: 32 warps per block
4. **Good Memory Coalescing**: Sequential access pattern

## Alternative Configurations (Not Recommended)

### 1. Fewer Threads per Block:
```cpp
threads_per_block = 256; // Only 8 warps
```
**Issues**:
- Lower occupancy (2 blocks per SM vs 8)
- More blocks needed (4x more)
- Underutilized SM resources

### 2. More Threads per Block:
```cpp
threads_per_block = 2048; // Not possible (max 1024)
```
**Issues**:
- Exceeds hardware limits
- Compilation error

### 3. Different Block Mapping:
```cpp
// Multiple rows per block (complex implementation)
blocks_per_grid = batch_size / rows_per_block;
```
**Issues**:
- Complex memory access patterns
- Potential bank conflicts
- Reduced parallelism

## Advanced Optimizations

### 1. For Larger Dimensions (>1024):
```cpp
// Each thread handles multiple elements
int elements_per_thread = (dim + threads_per_block - 1) / threads_per_block;
```

### 2. For Smaller Batch Sizes:
```cpp
// Multiple rows per block to increase occupancy
int rows_per_block = max(1, 1728 / batch_size);
blocks_per_grid = (batch_size + rows_per_block - 1) / rows_per_block;
```

### 3. Shared Memory Optimization:
```cpp
// Use different shared memory sizes for different phases
size_t shared_mem_max = threads_per_block * sizeof(float);
size_t shared_mem_sum = threads_per_block * sizeof(float);
```

## Monitoring and Profiling

### Key Metrics to Monitor:
1. **Occupancy**: Should be >50%
2. **Memory Bandwidth**: Compare to peak (777.6 GB/s)
3. **Compute Utilization**: Should match workload
4. **Warp Efficiency**: Should be close to 100%

### Profiling Tools:
```bash
# NVIDIA Nsight Compute
ncu --metrics gpu__time_duration,sm__throughput.avg.pct_of_peak_active_elapsed ./softmax_optimized

# NVIDIA Nsight Systems  
nsys profile --trace=cuda ./softmax_optimized
```

## Best Practices Summary

1. **Use maximum threads per block** when problem size allows
2. **Map one block per batch element** for softmax
3. **Ensure sufficient blocks** to utilize all SMs
4. **Monitor occupancy** using profiling tools
5. **Balance memory and compute** based on workload
6. **Consider data types** (FP16 for 2x memory bandwidth)
7. **Profile regularly** to identify bottlenecks

## Conclusion

For the current softmax implementation with batch_size=49,152 and dim=1,024:
- **Optimal threads_per_block**: 1024 (maximum for A100)
- **Optimal blocks_per_grid**: 49,152 (batch_size)
- **Achieved occupancy**: 800% (8 blocks per SM)
- **Performance**: 159.46x speedup with 10% memory bandwidth utilization

The configuration is compute-optimal but memory-bound, which is typical for softmax operations.
