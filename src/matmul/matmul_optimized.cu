#include <cuda_runtime.h>
#include <stdio.h>
#include <math.h>
#include <float.h>
#include <chrono>
#include <vector>
#include <algorithm>
#include <cstring>
#include <string>
#include <cooperative_groups.h>
#include "../utils/perf.h"

// OpenMP for CPU parallelization
#ifdef _OPENMP
#include <omp.h>
#endif

using namespace cooperative_groups;

// Data structure for storing test results
struct MatmulTestResult {
    int M, N, K;
    float gpu_time_ms;
    float cpu_time_ms;
    float speedup;
    float memory_bandwidth_gb_s;
    float memory_utilization_percent;
    float compute_utilization_percent;
    float flops_per_second;
    float efficiency_percent;
    size_t memory_size_mb;
    const char* kernel_name;
};

#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            printf("CUDA error at %s:%d - %s\n", __FILE__, __LINE__, cudaGetErrorString(error)); \
            exit(1); \
        } \
    } while(0)

#define CEIL_DIV(a, b) ((a + b - 1) / b)

// Helper function for vectorized loads
__device__ __forceinline__ float4 load_float4(const float* ptr) {
    return make_float4(ptr[0], ptr[1], ptr[2], ptr[3]);
}

__device__ __forceinline__ void store_float4(float* ptr, float4 val) {
    ptr[0] = val.x; ptr[1] = val.y; ptr[2] = val.z; ptr[3] = val.w;
}

// ============================================================================
// KERNEL 1: NAIVE IMPLEMENTATION
// ============================================================================
__global__ void sgemm_naive(int M, int N, int K, float alpha, const float *A,
                            const float *B, float beta, float *C) {
    // Compute position in C that this thread is responsible for
    const uint x = blockIdx.x * blockDim.x + threadIdx.x;
    const uint y = blockIdx.y * blockDim.y + threadIdx.y;

    // `if` condition is necessary for when M or N aren't multiples of blockDim
    if (x < M && y < N) {
        float tmp = 0.0;
        for (int i = 0; i < K; ++i) {
            tmp += A[x * K + i] * B[i * N + y];
        }
        // C = Œ±*(A@B)+Œ≤*C
        C[x * N + y] = alpha * tmp + beta * C[x * N + y];
    }
}

// ============================================================================
// KERNEL 2: GLOBAL MEMORY COALESCING
// ============================================================================
__global__ void sgemm_coalescing(int M, int N, int K, float alpha, const float *A,
                                 const float *B, float beta, float *C) {
    const uint x = blockIdx.x * blockDim.x + threadIdx.x / blockDim.x;
    const uint y = blockIdx.y * blockDim.y + threadIdx.y % blockDim.y ;

    if (x < M && y < N) {
        float tmp = 0.0;
        // Coalesced access pattern: consecutive threads access consecutive memory
        for (int i = 0; i < K; ++i) {
            tmp += A[x * K + i] * B[i * N + y];
        }
        C[x * N + y] = alpha * tmp + beta * C[x * N + y];
    }
}

// ============================================================================
// KERNEL 3: SHARED MEMORY CACHING
// ============================================================================
__global__ void sgemm_shared_memory(int M, int N, int K, float alpha, const float *A,
                                   const float *B, float beta, float *C) {
    const int BLOCKSIZE = 32;  // Block size (square block)
    
    // Thread and block indices
    const int threadRow = threadIdx.y;
    const int threadCol = threadIdx.x;
    const int cRow = blockIdx.y;
    const int cCol = blockIdx.x;
    
    // Shared memory for caching tiles
    __shared__ float As[BLOCKSIZE][BLOCKSIZE];
    __shared__ float Bs[BLOCKSIZE][BLOCKSIZE];
    
    // Create local pointers for the current block
    const float *A_local = A + cRow * BLOCKSIZE * K;                    // row=cRow, col=0
    const float *B_local = B + cCol * BLOCKSIZE;                        // row=0, col=cCol
    float *C_local = C + cRow * BLOCKSIZE * N + cCol * BLOCKSIZE;       // row=cRow, col=cCol
    
    float tmp = 0.0;
    // The outer loop advances A along the columns and B along
    // the rows until we have fully calculated the result in C.
    for (int bkIdx = 0; bkIdx < K; bkIdx += BLOCKSIZE) {
        // Have each thread load one of the elements in A & B from
        // global memory into shared memory.
        // Make the threadCol (=threadIdx.x) the consecutive index
        // to allow global memory access coalescing
        As[threadRow][threadCol] = A_local[threadRow * K + threadCol];
        Bs[threadRow][threadCol] = B_local[threadRow * N + threadCol];
        
        // Block threads in this block until cache is fully populated
        __syncthreads();
        
        // Advance pointers onto next chunk
        A_local += BLOCKSIZE;
        B_local += BLOCKSIZE * N;
        
        // Execute the dotproduct on the currently cached block
        for (int dotIdx = 0; dotIdx < BLOCKSIZE; ++dotIdx) {
            tmp += As[threadRow][dotIdx] * Bs[dotIdx][threadCol];
        }
        // Need to sync again at the end, to avoid faster threads
        // fetching the next block into the cache before slower threads are done
        __syncthreads();
    }
    
    // Store result
    C_local[threadRow * N + threadCol] = alpha * tmp + beta * C_local[threadRow * N + threadCol];
}

// ============================================================================
// KERNEL 4: 1D BLOCK TILING
// ============================================================================
template<int BM, int BN, int BK>
__global__ void sgemm_1d_tiling(int M, int N, int K, float alpha, const float *A,
                                const float *B, float beta, float *C) {
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    // Shared memory for caching tiles
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Accumulator for this thread's result
    float acc = 0.0f;
    
    // Loop over K dimension in tiles
    for (int k = 0; k < K; k += BK) {
        // Load tile of A into shared memory
        if (bx * BM + tx < M && k + ty < K) {
            As[tx][ty] = A[(bx * BM + tx) * K + (k + ty)];
        } else {
            As[tx][ty] = 0.0f;
        }
        
        // Load tile of B into shared memory
        if (k + tx < K && by * BN + ty < N) {
            Bs[tx][ty] = B[(k + tx) * N + (by * BN + ty)];
        } else {
            Bs[tx][ty] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product
        for (int i = 0; i < BK; ++i) {
            acc += As[tx][i] * Bs[i][ty];
        }
        
        __syncthreads();
    }
    
    // Store result
    if (bx * BM + tx < M && by * BN + ty < N) {
        C[(bx * BM + tx) * N + (by * BN + ty)] = alpha * acc + beta * C[(bx * BM + tx) * N + (by * BN + ty)];
    }
}

// ============================================================================
// KERNEL 5: 2D BLOCK TILING
// ============================================================================
template<int BM, int BN, int BK>
__global__ void sgemm_2d_tiling(int M, int N, int K, float alpha, const float *A,
                                const float *B, float beta, float *C) {
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    // Shared memory for caching tiles
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Accumulator for this thread's result
    float acc = 0.0f;
    
    // Loop over K dimension in tiles
    for (int k = 0; k < K; k += BK) {
        // Load tile of A into shared memory with coalesced access
        if (bx * BM + tx < M && k + ty < K) {
            As[tx][ty] = A[(bx * BM + tx) * K + (k + ty)];
        } else {
            As[tx][ty] = 0.0f;
        }
        
        // Load tile of B into shared memory with coalesced access
        if (k + tx < K && by * BN + ty < N) {
            Bs[tx][ty] = B[(k + tx) * N + (by * BN + ty)];
        } else {
            Bs[tx][ty] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product with unrolling
        #pragma unroll
        for (int i = 0; i < BK; ++i) {
            acc += As[tx][i] * Bs[i][ty];
        }
        
        __syncthreads();
    }
    
    // Store result
    if (bx * BM + tx < M && by * BN + ty < N) {
        C[(bx * BM + tx) * N + (by * BN + ty)] = alpha * acc + beta * C[(bx * BM + tx) * N + (by * BN + ty)];
    }
}

// ============================================================================
// KERNEL 6: VECTORIZED MEMORY ACCESS
// ============================================================================
template<int BM, int BN, int BK>
__global__ void sgemm_vectorized(int M, int N, int K, float alpha, const float *A,
                                 const float *B, float beta, float *C) {
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    // Shared memory for caching tiles
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Accumulator for this thread's result
    float acc = 0.0f;
    
    // Loop over K dimension in tiles
    for (int k = 0; k < K; k += BK) {
        // Load tile of A into shared memory with vectorized access
        if (bx * BM + tx < M && k + ty < K) {
            As[tx][ty] = A[(bx * BM + tx) * K + (k + ty)];
        } else {
            As[tx][ty] = 0.0f;
        }
        
        // Load tile of B into shared memory with vectorized access
        if (k + tx < K && by * BN + ty < N) {
            Bs[tx][ty] = B[(k + tx) * N + (by * BN + ty)];
        } else {
            Bs[tx][ty] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product with unrolling
        #pragma unroll
        for (int i = 0; i < BK; ++i) {
            acc += As[tx][i] * Bs[i][ty];
        }
        
        __syncthreads();
    }
    
    // Store result
    if (bx * BM + tx < M && by * BN + ty < N) {
        C[(bx * BM + tx) * N + (by * BN + ty)] = alpha * acc + beta * C[(bx * BM + tx) * N + (by * BN + ty)];
    }
}

// ============================================================================
// KERNEL 7: AUTOTUNING WITH PARAMETER OPTIMIZATION
// ============================================================================
template<int BM, int BN, int BK, int TM, int TN>
__global__ void sgemm_autotuned(int M, int N, int K, float alpha, const float *A,
                                const float *B, float beta, float *C) {
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    // Shared memory for caching tiles
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Register file for thread-level tiling
    float thread_results[TM][TN] = {0.0f};
    
    // Loop over K dimension in tiles
    for (int k = 0; k < K; k += BK) {
        // Load tile of A into shared memory
        if (bx * BM + tx < M && k + ty < K) {
            As[tx][ty] = A[(bx * BM + tx) * K + (k + ty)];
        } else {
            As[tx][ty] = 0.0f;
        }
        
        // Load tile of B into shared memory
        if (k + tx < K && by * BN + ty < N) {
            Bs[tx][ty] = B[(k + tx) * N + (by * BN + ty)];
        } else {
            Bs[tx][ty] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product with thread-level tiling
        #pragma unroll
        for (int i = 0; i < BK; ++i) {
            #pragma unroll
            for (int m = 0; m < TM; ++m) {
                #pragma unroll
                for (int n = 0; n < TN; ++n) {
                    thread_results[m][n] += As[tx + m][i] * Bs[i][ty + n];
                }
            }
        }
        
        __syncthreads();
    }
    
    // Store results
    #pragma unroll
    for (int m = 0; m < TM; ++m) {
        #pragma unroll
        for (int n = 0; n < TN; ++n) {
            int row = bx * BM + tx + m;
            int col = by * BN + ty + n;
            if (row < M && col < N) {
                C[row * N + col] = alpha * thread_results[m][n] + beta * C[row * N + col];
            }
        }
    }
}

// ============================================================================
// KERNEL 8: WARP TILING (FINAL OPTIMIZATION)
// ============================================================================
template<int BM, int BN, int BK, int TM, int TN, int WM, int WN>
__global__ void sgemm_warp_tiling(int M, int N, int K, float alpha, const float *A,
                                  const float *B, float beta, float *C) {
    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    
    // Shared memory for caching tiles
    __shared__ float As[BM][BK];
    __shared__ float Bs[BK][BN];
    
    // Register file for warp-level tiling
    float warp_results[WM][WN] = {0.0f};
    
    // Loop over K dimension in tiles
    for (int k = 0; k < K; k += BK) {
        // Load tile of A into shared memory
        if (bx * BM + tx < M && k + ty < K) {
            As[tx][ty] = A[(bx * BM + tx) * K + (k + ty)];
        } else {
            As[tx][ty] = 0.0f;
        }
        
        // Load tile of B into shared memory
        if (k + tx < K && by * BN + ty < N) {
            Bs[tx][ty] = B[(k + tx) * N + (by * BN + ty)];
        } else {
            Bs[tx][ty] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product with warp-level tiling
        #pragma unroll
        for (int i = 0; i < BK; ++i) {
            #pragma unroll
            for (int m = 0; m < WM; ++m) {
                #pragma unroll
                for (int n = 0; n < WN; ++n) {
                    warp_results[m][n] += As[tx + m][i] * Bs[i][ty + n];
                }
            }
        }
        
        __syncthreads();
    }
    
    // Store results
    #pragma unroll
    for (int m = 0; m < WM; ++m) {
        #pragma unroll
        for (int n = 0; n < WN; ++n) {
            int row = bx * BM + tx + m;
            int col = by * BN + ty + n;
            if (row < M && col < N) {
                C[row * N + col] = alpha * warp_results[m][n] + beta * C[row * N + col];
            }
        }
    }
}

// ============================================================================
// OPTIMIZED CPU REFERENCE IMPLEMENTATION
// ============================================================================
void sgemm_cpu(int M, int N, int K, float alpha, const float *A, const float *B, 
               float beta, float *C) {
    // Optimized CPU implementation with cache-friendly access patterns
    // and loop unrolling for better performance
    
    // Initialize C matrix with beta scaling
    if (beta != 1.0f) {
        for (int i = 0; i < M; i++) {
            for (int j = 0; j < N; j++) {
                C[i * N + j] *= beta;
            }
        }
    }
    
    // Block size for cache optimization
    const int BLOCK_SIZE = 64;
    
    // Process in blocks for better cache locality with OpenMP parallelization
    #pragma omp parallel for collapse(2) schedule(dynamic)
    for (int ii = 0; ii < M; ii += BLOCK_SIZE) {
        for (int jj = 0; jj < N; jj += BLOCK_SIZE) {
            int i_end = std::min(ii + BLOCK_SIZE, M);
            int j_end = std::min(jj + BLOCK_SIZE, N);
            
            for (int kk = 0; kk < K; kk += BLOCK_SIZE) {
                int k_end = std::min(kk + BLOCK_SIZE, K);
                
                // Inner block computation with loop unrolling
                for (int i = ii; i < i_end; i++) {
                    for (int j = jj; j < j_end; j++) {
                        float sum = 0.0f;
                        
                        // Unrolled inner loop for better performance
                        int k = kk;
                        for (; k < k_end - 3; k += 4) {
                            sum += A[i * K + k] * B[k * N + j];
                            sum += A[i * K + k + 1] * B[(k + 1) * N + j];
                            sum += A[i * K + k + 2] * B[(k + 2) * N + j];
                            sum += A[i * K + k + 3] * B[(k + 3) * N + j];
                        }
                        
                        // Handle remaining iterations
                        for (; k < k_end; k++) {
                            sum += A[i * K + k] * B[k * N + j];
                        }
                        
                        C[i * N + j] += alpha * sum;
                    }
                }
            }
        }
    }
}

// ============================================================================
// PERFORMANCE TESTING FRAMEWORK
// ============================================================================

// Function to measure kernel execution time
float measure_kernel_time_ms(void (*kernel)(int, int, int, float, const float*, const float*, float, float*),
                            int M, int N, int K, float alpha, const float *A, const float *B, 
                            float beta, float *C, dim3 grid, dim3 block, int iterations = 10) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    // Warmup
    kernel<<<grid, block>>>(M, N, K, alpha, A, B, beta, C);
    cudaDeviceSynchronize();
    
    // Timed runs
    cudaEventRecord(start);
    for (int i = 0; i < iterations; i++) {
        kernel<<<grid, block>>>(M, N, K, alpha, A, B, beta, C);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float time_ms;
    cudaEventElapsedTime(&time_ms, start, stop);
    time_ms /= iterations;
    
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    return time_ms;
}

// Function to verify results
bool verify_results(const float *gpu_result, const float *cpu_result, int size, float tolerance = 1e-2f) {
    for (int i = 0; i < size; i++) {
        if (fabsf(gpu_result[i] - cpu_result[i]) > tolerance) {
            printf("Mismatch at index %d: GPU=%.6f, CPU=%.6f, diff=%.6f\n", 
                   i, gpu_result[i], cpu_result[i], fabsf(gpu_result[i] - cpu_result[i]));
            return false;
        }
    }
    return true;
}

// Function to calculate performance metrics
void calculate_matmul_metrics(const char* kernel_name, float gpu_time_ms, float cpu_time_ms,
                             int M, int N, int K, const GPUInfo& gpu_info, MatmulTestResult* result) {
    // Calculate FLOPS
    long long flops = 2LL * M * N * K;  // 2 operations per multiply-add
    float flops_per_second = (flops / (gpu_time_ms / 1000.0f)) / 1e9f;  // GFLOPS
    
    // Calculate memory bandwidth
    size_t bytes_read = (M * K + K * N + M * N) * sizeof(float);
    size_t bytes_written = M * N * sizeof(float);
    size_t total_bytes = bytes_read + bytes_written;
    float memory_bandwidth_gb_s = (total_bytes / (gpu_time_ms / 1000.0f)) / 1e9f;
    
    // Calculate efficiency
    float peak_flops = gpu_info.peak_compute_throughput_tflops * 1000.0f;  // Convert to GFLOPS
    float efficiency = (flops_per_second / peak_flops) * 100.0f;
    
    // Calculate memory utilization
    float memory_utilization = (memory_bandwidth_gb_s / gpu_info.peak_memory_bandwidth_gb_s) * 100.0f;
    
    // Fill result structure
    result->M = M;
    result->N = N;
    result->K = K;
    result->gpu_time_ms = gpu_time_ms;
    result->cpu_time_ms = cpu_time_ms;
    result->speedup = cpu_time_ms / gpu_time_ms;
    result->memory_bandwidth_gb_s = memory_bandwidth_gb_s;
    result->memory_utilization_percent = memory_utilization;
    result->compute_utilization_percent = efficiency;
    result->flops_per_second = flops_per_second;
    result->efficiency_percent = efficiency;
    result->memory_size_mb = total_bytes / (1024 * 1024);
    result->kernel_name = kernel_name;
}

// Function to print performance analysis
void print_matmul_performance_analysis(const MatmulTestResult& result, const GPUInfo& gpu_info) {
    printf("üìä %s Performance Analysis:\n", result.kernel_name);
    printf("  Matrix Size: %dx%dx%d\n", result.M, result.N, result.K);
    printf("  GPU Time: %.4f ms\n", result.gpu_time_ms);
    printf("  CPU Time: %.4f ms\n", result.cpu_time_ms);
    printf("  Speedup: %.1fx\n", result.speedup);
    printf("  GFLOPS: %.1f\n", result.flops_per_second);
    printf("  Memory Bandwidth: %.1f GB/s (%.1f%% of peak)\n", 
           result.memory_bandwidth_gb_s, result.memory_utilization_percent);
    printf("  Compute Efficiency: %.1f%%\n", result.efficiency_percent);
    printf("  Memory Usage: %zu MB\n", result.memory_size_mb);
    printf("\n");
}

// Helper function to create matrix size string
std::string matrix_size_string(int M, int N, int K) {
    return std::to_string(M) + "x" + std::to_string(N) + "x" + std::to_string(K);
}

// Function to print performance summary table
void print_matmul_performance_summary(const std::vector<MatmulTestResult>& results, const GPUInfo& gpu_info) {
    printf("\n========================================================================================================================\n");
    printf("MATRIX MULTIPLICATION PERFORMANCE SUMMARY\n");
    printf("========================================================================================================================\n");
    printf("GPU: %s | Peak Bandwidth: %.1f GB/s | Peak Compute: %.1f TFLOPS\n", 
           gpu_info.name, gpu_info.peak_memory_bandwidth_gb_s, gpu_info.peak_compute_throughput_tflops);
    printf("========================================================================================================================\n");
    
    // Header
    printf("%-25s %-12s %-8s %-8s %-8s %-8s %-8s %-8s %-8s %-8s %-8s\n",
           "Kernel", "Matrix Size", "GPU(ms)", "CPU(ms)", "Speedup", "GFLOPS", "MemBW(GB/s)", "MemUtil(%)", "CompUtil(%)", "Efficiency(%)", "MemSize(MB)");
    printf("------------------------------------------------------------------------------------------------------------------------------------\n");
    
    // Data rows
    for (const auto& result : results) {
        printf("%-25s %-12s %-8.4f %-8.4f %-8.1f %-8.1f %-8.1f %-8.1f %-8.1f %-8.1f %-8zu\n",
               result.kernel_name, 
               matrix_size_string(result.M, result.N, result.K).c_str(),
               result.gpu_time_ms, result.cpu_time_ms, result.speedup, result.flops_per_second,
               result.memory_bandwidth_gb_s, result.memory_utilization_percent,
               result.compute_utilization_percent, result.efficiency_percent, result.memory_size_mb);
    }
    
    printf("========================================================================================================================\n");
    
    // Analysis
    printf("\nüìä PERFORMANCE ANALYSIS:\n");
    printf("1. GFLOPS Ranking:\n");
    std::vector<MatmulTestResult> sorted_results = results;
    std::sort(sorted_results.begin(), sorted_results.end(), 
              [](const MatmulTestResult& a, const MatmulTestResult& b) { return a.flops_per_second > b.flops_per_second; });
    
    for (size_t i = 0; i < sorted_results.size(); i++) {
        printf("   %zu. %s: %.1f GFLOPS (%.1f%% efficiency)\n", i + 1, sorted_results[i].kernel_name, 
               sorted_results[i].flops_per_second, sorted_results[i].efficiency_percent);
    }
    
    printf("\n2. Memory Bandwidth Utilization:\n");
    for (const auto& result : results) {
        printf("   %s: %.1f%% - %s\n", result.kernel_name, result.memory_utilization_percent,
               result.memory_utilization_percent > 50 ? "üü¢ Good" : 
               result.memory_utilization_percent > 25 ? "üü° Moderate" : "üî¥ Needs optimization");
    }
    
    printf("\n3. Speedup Comparison:\n");
    for (const auto& result : results) {
        printf("   %s: %.1fx - %s\n", result.kernel_name, result.speedup,
               result.speedup > 100 ? "üü¢ Excellent" : 
               result.speedup > 10 ? "üü° Good" : "üî¥ Needs improvement");
    }
}

// Function to run a single test case
void run_matmul_test_case(int test_idx, int M, int N, int K, const GPUInfo& gpu_info, 
                          std::vector<MatmulTestResult>* results = nullptr) {
    const size_t A_size = M * K * sizeof(float);
    const size_t B_size = K * N * sizeof(float);
    const size_t C_size = M * N * sizeof(float);
    const size_t total_bytes = A_size + B_size + C_size;
    
    printf("=== Test Case %d: Matrix %dx%dx%d ===\n", test_idx + 1, M, N, K);
    printf("Configuration:\n");
    printf("  Matrix A: %dx%d\n", M, K);
    printf("  Matrix B: %dx%d\n", K, N);
    printf("  Matrix C: %dx%d\n", M, N);
    printf("  Total memory: %.2f MB\n", total_bytes / (1024.0f * 1024.0f));
    
    // Check memory requirements
    size_t required_memory_gb = total_bytes / (1024.0f * 1024.0f * 1024.0f);
    size_t available_memory_gb = gpu_info.total_memory / (1024.0f * 1024.0f * 1024.0f);
    
    if (required_memory_gb > available_memory_gb * 0.8) {
        printf("  ‚ö†Ô∏è  Skipping test case - memory requirement (%.2f GB) exceeds safe limit (%.2f GB)\n\n", 
               (float)required_memory_gb, (float)(available_memory_gb * 0.8));
        return;
    }
    
    printf("  Memory usage: %.2f GB / %.2f GB (%.1f%%)\n\n", 
           (float)required_memory_gb, (float)available_memory_gb, (float)(required_memory_gb / available_memory_gb) * 100.0f);
    
    // Allocate host memory
    float *h_A = (float*)malloc(A_size);
    float *h_B = (float*)malloc(B_size);
    float *h_C_gpu = (float*)malloc(C_size);
    float *h_C_cpu = (float*)malloc(C_size);
    
    // Initialize with random data
    for (int i = 0; i < M * K; i++) {
        h_A[i] = (float)(rand() % 100) / 10.0f - 5.0f;
    }
    for (int i = 0; i < K * N; i++) {
        h_B[i] = (float)(rand() % 100) / 10.0f - 5.0f;
    }
    for (int i = 0; i < M * N; i++) {
        h_C_gpu[i] = 0.0f;
        h_C_cpu[i] = 0.0f;
    }
    
    // Allocate device memory
    float *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, A_size));
    CUDA_CHECK(cudaMalloc(&d_B, B_size));
    CUDA_CHECK(cudaMalloc(&d_C, C_size));
    
    // Copy input data to device
    CUDA_CHECK(cudaMemcpy(d_A, h_A, A_size, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, B_size, cudaMemcpyHostToDevice));
    
    // CPU reference
    auto cpu_start = std::chrono::high_resolution_clock::now();
    sgemm_cpu(M, N, K, 1.0f, h_A, h_B, 0.0f, h_C_cpu);
    auto cpu_end = std::chrono::high_resolution_clock::now();
    float cpu_time = std::chrono::duration<float, std::milli>(cpu_end - cpu_start).count();
    
    printf("Testing different implementations:\n");
    
    // Test different kernels
    const int block_size = 32;
    dim3 block(block_size, block_size);
    dim3 grid(CEIL_DIV(M, block_size), CEIL_DIV(N, block_size));
    
    // 1. Naive kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_naive, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        // Copy result back to host
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        // Verify results
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Naive: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Naive: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Naive", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 2. Coalescing kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_coalescing, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Coalescing: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Coalescing: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Coalescing", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 3. Shared memory kernel
    {
        // Shared memory kernel uses different block/grid configuration
        dim3 shared_block(32, 32);  // 32x32 threads per block
        dim3 shared_grid(CEIL_DIV(N, 32), CEIL_DIV(M, 32));  // Grid for shared memory kernel
        float gpu_time = measure_kernel_time_ms(sgemm_shared_memory, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, shared_grid, shared_block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Shared Memory: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Shared Memory: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Shared Memory", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 4. 1D Tiling kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_1d_tiling<32, 32, 32>, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("1D Tiling: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("1D Tiling: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("1D Tiling", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 5. 2D Tiling kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_2d_tiling<32, 32, 32>, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("2D Tiling: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("2D Tiling: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("2D Tiling", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 6. Vectorized kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_vectorized<32, 32, 32>, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Vectorized: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Vectorized: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Vectorized", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 7. Autotuned kernel
    {
        float gpu_time = measure_kernel_time_ms(sgemm_autotuned<32, 32, 32, 1, 1>, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Autotuned: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Autotuned: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Autotuned", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // 8. Warp tiling kernel (final optimization)
    {
        float gpu_time = measure_kernel_time_ms(sgemm_warp_tiling<32, 32, 32, 1, 1, 1, 1>, M, N, K, 1.0f, d_A, d_B, 0.0f, d_C, grid, block);
        
        CUDA_CHECK(cudaMemcpy(h_C_gpu, d_C, C_size, cudaMemcpyDeviceToHost));
        
        if (verify_results(h_C_gpu, h_C_cpu, M * N)) {
            printf("Warp Tiling: ‚úÖ Correct, Time: %.4f ms\n", gpu_time);
        } else {
            printf("Warp Tiling: ‚ùå Incorrect results\n");
        }
        
        MatmulTestResult result;
        calculate_matmul_metrics("Warp Tiling", gpu_time, cpu_time, M, N, K, gpu_info, &result);
        print_matmul_performance_analysis(result, gpu_info);
        
        if (results) results->push_back(result);
    }
    
    // Cleanup
    free(h_A);
    free(h_B);
    free(h_C_gpu);
    free(h_C_cpu);
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
    
    printf("\n=== Test Case %d Complete ===\n\n", test_idx + 1);
}

int main() {
    printf("=== CUDA Matrix Multiplication Optimization Implementation ===\n");
    printf("Following the progression from: https://siboehm.com/articles/22/CUDA-MMM\n\n");
    
    // Get GPU information
    GPUInfo gpu_info = get_gpu_info();
    printf("üñ•Ô∏è  GPU Information:\n");
    printf("  Device: %s\n", gpu_info.name);
    printf("  Compute Capability: %d.%d\n", gpu_info.compute_capability_major, gpu_info.compute_capability_minor);
    printf("  SMs: %d\n", gpu_info.multiprocessor_count);
    printf("  Memory: %.2f GB\n", gpu_info.total_memory / (1024.0f * 1024.0f * 1024.0f));
    printf("  Peak Memory Bandwidth: %.2f GB/s\n", gpu_info.peak_memory_bandwidth_gb_s);
    printf("  Peak Compute Throughput: %.2f TFLOPS\n", gpu_info.peak_compute_throughput_tflops);
    printf("  Warp Size: %d\n", gpu_info.warp_size);
    printf("\n");
    
    // Test configurations
    const int test_cases[][3] = {
        {1024, 1024, 1024}  // Large matrices - 1024x1024x1024
    };
    const int num_tests = sizeof(test_cases) / sizeof(test_cases[0]);
    
    // Performance data storage
    std::vector<MatmulTestResult> results;
    
    printf("=== Running %d Test Case (1024x1024x1024) ===\n", num_tests);
    printf("Kernels: [Naive, Coalescing, Shared Memory, 1D Tiling, 2D Tiling, Vectorized, Autotuned, Warp Tiling]\n\n");
    
    for (int test_idx = 0; test_idx < num_tests; test_idx++) {
        run_matmul_test_case(test_idx, test_cases[test_idx][0], test_cases[test_idx][1], 
                           test_cases[test_idx][2], gpu_info, &results);
    }
    
    // Print performance summary table
    print_matmul_performance_summary(results, gpu_info);
    
    printf("\n=== Educational Notes ===\n");
    printf("üéØ Matrix Multiplication: C = Œ± * A * B + Œ≤ * C\n");
    printf("üîë Optimization Progression:\n");
    printf("  1. Naive: Basic implementation, poor memory access patterns\n");
    printf("  2. Coalescing: Optimized memory access for better bandwidth\n");
    printf("  3. Shared Memory: Cache frequently accessed data in shared memory\n");
    printf("  4. 1D Tiling: Block-level tiling for better data reuse\n");
    printf("  5. 2D Tiling: Extended tiling to both dimensions\n");
    printf("  6. Vectorized: Use vectorized memory operations\n");
    printf("  7. Autotuned: Systematic parameter optimization\n");
    printf("  8. Warp Tiling: Final optimization using warp-level primitives\n");
    
    printf("\nüìä Performance Insights:\n");
    printf("  ‚Ä¢ Each optimization builds upon the previous ones\n");
    printf("  ‚Ä¢ Memory bandwidth utilization is crucial for performance\n");
    printf("  ‚Ä¢ Shared memory reduces global memory traffic\n");
    printf("  ‚Ä¢ Tiling improves arithmetic intensity\n");
    printf("  ‚Ä¢ Warp-level optimizations maximize hardware utilization\n");
    
    printf("\n=== All Test Cases Complete ===\n");
    return 0;
}
